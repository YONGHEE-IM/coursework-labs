## 실습 1
import numpy as np

class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.position = 0

    def push(self, transition):
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = transition
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        return np.random.choice(self.buffer, batch_size, replace=False)

    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    def __init__(self, q_network, target_network, optimizer, replay_buffer, batch_size, gamma):
        self.q_network = q_network
        self.target_network = target_network
        self.optimizer = optimizer
        self.replay_buffer = replay_buffer
        self.batch_size = batch_size
        self.gamma = gamma

    def train(self):
        # 1A: Replay Buffer에서 배치를 샘플링
        if len(self.replay_buffer) < self.batch_size:
            return  # 학습할 데이터가 충분하지 않으면 종료
        batch = self.replay_buffer.sample(self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        # numpy 배열로 변환
        states = np.stack(states)
        actions = np.array(actions)
        rewards = np.array(rewards)
        next_states = np.stack(next_states)
        dones = np.array(dones, dtype=np.float32)

        # 1B: Q 값을 계산
        current_q_values = self.q_network.predict(states)  # Q(s, a)
        target_q_values = self.target_network.predict(next_states)  # Q'(s', a')
        max_next_q_values = np.max(target_q_values, axis=1)

        # 종료 상태는 보상만 고려
        targets = rewards + (1 - dones) * self.gamma * max_next_q_values

        # 선택된 행동의 Q 값을 업데이트
        current_q_values[np.arange(self.batch_size), actions] = targets

        # Q 네트워크 업데이트
        loss = self.q_network.train_on_batch(states, current_q_values)
        return loss

    def simulate_action(self, raw_state, epsilon):
        if np.random.rand() < epsilon:  # 랜덤 행동 선택
            action = np.random.choice(self.q_network.output_shape[-1])
        else:  # Q 네트워크를 통해 행동 선택
            processed_state = self.get_current_state(raw_state)
            q_values = self.q_network.predict(processed_state[None, :])
            action = np.argmax(q_values[0])
        return action

    def add_raw_state(self, raw_state):
        # 상태 전처리 추가
        pass

    def get_current_state(self, raw_state):
        # 현재 상태 반환
        pass

# 2A, 2B, 2C 구현
# 환경과 상호작용
raw_state = environment.reset()  # 환경 초기화
done = False
while not done:
    action = agent.simulate_action(raw_state, epsilon=0.1)  # 행동 선택

    # 2A: 환경에 행동을 적용하여 결과를 얻음
    next_raw_state, reward, done, _ = environment.step(action)

    # 2B: 다음 상태를 전처리
    agent.add_raw_state(next_raw_state)
    next_state = agent.get_current_state(next_raw_state)

    # 2C: 5-tuple을 Replay Buffer에 저장
    current_state = agent.get_current_state(raw_state)
    agent.replay_buffer.push((current_state, action, reward, next_state, done))

    # 상태 업데이트
    raw_state = next_raw_state

# DQN 학습
loss = agent.train()
print(f"Training loss: {loss}")


## 실습 2
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np


class PolicyNetwork(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(PolicyNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )
        self.log_std = nn.Parameter(torch.zeros(output_dim))

    def forward(self, x):
        mu = self.fc(x)
        std = torch.exp(self.log_std)
        return mu, std


class ValueNetwork(nn.Module):
    def __init__(self, input_dim):
        super(ValueNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.fc(x)


class PGAgent:
    def __init__(self, state_dim, action_dim, gamma=0.99):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.value = ValueNetwork(state_dim)
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=1e-3)
        self.value_optimizer = optim.Adam(self.value.parameters(), lr=1e-3)
        self.gamma = gamma
        self.saved_log_probs = []
        self.rewards = []

    def act(self, state):
        state = torch.FloatTensor(state)
        
        # *1A*: Policy Network로부터 평균과 표준 편차를 받아 정규 분포를 생성
        mu, std = self.policy(state)
        distribution = torch.distributions.Normal(mu, std)
        
        # 행동 샘플링 및 로그 확률 저장
        action = distribution.sample()
        log_prob = distribution.log_prob(action).sum()
        self.saved_log_probs.append(log_prob)
        return action.detach().numpy()

    def train(self):
        # *1B*: 각 스텝의 누적 보상 계산
        R = 0
        returns = []
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)
        returns = torch.tensor(returns)

        # *1C*: 정책 네트워크 학습을 위한 목적 함수 계산
        loss = []
        for log_prob, R in zip(self.saved_log_probs, returns):
            loss.append(-log_prob * R)  # Gradient ascent
        policy_loss = torch.stack(loss).sum()

        # 정책 네트워크 업데이트
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()

        # Critic 업데이트 (Value Network)
        values = self.value(torch.FloatTensor(self.rewards))
        value_loss = (values - returns).pow(2).mean()
        self.value_optimizer.zero_grad()
        value_loss.backward()
        self.value_optimizer.step()

        # 메모리 초기화
        self.saved_log_probs = []
        self.rewards = []


class A2CAgent:
    def __init__(self, state_dim, action_dim, gamma=0.99):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.value = ValueNetwork(state_dim)
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=1e-3)
        self.value_optimizer = optim.Adam(self.value.parameters(), lr=1e-3)
        self.gamma = gamma

    def act(self, state):
        state = torch.FloatTensor(state)
        mu, std = self.policy(state)
        distribution = torch.distributions.Normal(mu, std)
        action = distribution.sample()
        log_prob = distribution.log_prob(action).sum()
        return action.detach().numpy(), log_prob

    def train(self, trajectory):
        states, actions, rewards, next_states, dones, log_probs = trajectory

        # *2A*: Critic으로 현재 상태와 다음 상태의 V-value 계산
        states = torch.FloatTensor(states)
        next_states = torch.FloatTensor(next_states)
        rewards = torch.FloatTensor(rewards)
        dones = torch.FloatTensor(dones)
        
        values = self.value(states).squeeze()
        next_values = self.value(next_states).squeeze()

        # *2B*: Advantage 값 계산
        Q_values = rewards + self.gamma * next_values * (1 - dones)
        advantages = Q_values - values

        # *2C*: Advantage 값을 활용한 목적 함수 계산 및 Actor 학습
        policy_loss = (-log_probs * advantages.detach()).mean()
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()

        # Critic 학습 (Value Loss 계산)
        value_loss = advantages.pow(2).mean()
        self.value_optimizer.zero_grad()
        value_loss.backward()
        self.value_optimizer.step()


